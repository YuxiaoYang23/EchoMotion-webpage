<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <!-- Primary Meta Tags -->
  <meta name="title" content="EchoMotion: Unified Human Video and Motion Generation via Dual-Modality Diffusion Transformer">
  <meta name="description" content="EchoMotion introduces a novel dual-modality diffusion transformer framework for unified human video and motion generation, enabling joint learning of appearance and kinematics.">
  <meta name="keywords" content="human motion generation, video generation, diffusion transformer, motion synthesis, computer vision, deep learning, generative models">
  <meta name="author" content="Yuxiao Yang, Hualian Sheng, Sijia Cai, Jing Lin, Jiahao Wang, Bing Deng, Junzhe Lu, Haoqian Wang, Jieping Ye">
  <meta name="robots" content="index, follow">
  <meta name="language" content="English">
  
  <!-- Open Graph / Facebook -->
  <meta property="og:type" content="article">
  <meta property="og:site_name" content="Tsinghua University & Alibaba Group">
  <meta property="og:title" content="EchoMotion: Unified Human Video and Motion Generation via Dual-Modality Diffusion Transformer">
  <meta property="og:description" content="EchoMotion introduces a novel dual-modality diffusion transformer framework for unified human video and motion generation, enabling joint learning of appearance and kinematics.">
  <meta property="og:image:alt" content="EchoMotion - Research Preview">
  <meta property="article:published_time" content="2024-01-01T00:00:00.000Z">
  <meta property="article:author" content="Yuxiao Yang">
  <meta property="article:section" content="Research">
  <meta property="article:tag" content="human motion generation">
  <meta property="article:tag" content="video generation">

  <!-- Twitter -->
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:title" content="EchoMotion: Unified Human Video and Motion Generation via Dual-Modality Diffusion Transformer">
  <meta name="twitter:description" content="EchoMotion introduces a novel dual-modality diffusion transformer framework for unified human video and motion generation, enabling joint learning of appearance and kinematics.">
  <meta name="twitter:image:alt" content="EchoMotion - Research Preview">

  <!-- Academic/Research Specific -->
  <meta name="citation_title" content="EchoMotion: Unified Human Video and Motion Generation via Dual-Modality Diffusion Transformer">
  <meta name="citation_author" content="Yang, Yuxiao">
  <meta name="citation_author" content="Sheng, Hualian">
  <meta name="citation_author" content="Cai, Sijia">
  <meta name="citation_author" content="Lin, Jing">
  <meta name="citation_author" content="Wang, Jiahao">
  <meta name="citation_author" content="Deng, Bing">
  <meta name="citation_author" content="Lu, Junzhe">
  <meta name="citation_author" content="Wang, Haoqian">
  <meta name="citation_author" content="Ye, Jieping">
  <meta name="citation_publication_date" content="2024">
  
  <!-- Additional SEO -->
  <meta name="theme-color" content="#2563eb">
  <meta name="msapplication-TileColor" content="#2563eb">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="default">
  
  <!-- Preconnect for performance -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link rel="preconnect" href="https://ajax.googleapis.com">
  <link rel="preconnect" href="https://documentcloud.adobe.com">
  <link rel="preconnect" href="https://cdn.jsdelivr.net">


  <title>EchoMotion: Unified Human Video and Motion Generation via Dual-Modality Diffusion Transformer</title>

  
  <!-- Favicon and App Icons -->
  <link rel="icon" type="image/x-icon" href="static/images/logo.ico">
  <link rel="apple-touch-icon" href="static/images/logo.ico">
  
  <!-- Critical CSS - Load synchronously -->
  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/index.css">
  
  <!-- Non-critical CSS - Load asynchronously -->
  <link rel="preload" href="static/css/bulma-carousel.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/bulma-slider.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/fontawesome.all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  
  <!-- Fallback for browsers that don't support preload -->
  <noscript>
    <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  </noscript>
  
  <!-- Fonts - Optimized loading -->
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;800&display=swap" rel="stylesheet">
  
  <!-- Defer non-critical JavaScript -->
  <script defer src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script defer src="static/js/bulma-carousel.min.js"></script>
  <script defer src="static/js/bulma-slider.min.js"></script>
  <script defer src="static/js/index.js"></script>
  
  <!-- Structured Data for Academic Papers -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "ScholarlyArticle",
    "headline": "EchoMotion: Unified Human Video and Motion Generation via Dual-Modality Diffusion Transformer",
    "description": "EchoMotion introduces a novel dual-modality diffusion transformer framework for unified human video and motion generation, enabling joint learning of appearance and kinematics.",
    "author": [
      {
        "@type": "Person",
        "name": "Yuxiao Yang",
        "affiliation": {
          "@type": "Organization",
          "name": "Tsinghua University"
        }
      },
      {
        "@type": "Person",
        "name": "Hualian Sheng",
        "affiliation": {
          "@type": "Organization",
          "name": "Alibaba Group"
        }
      }
    ],
    "datePublished": "2024",
    "keywords": ["human motion generation", "video generation", "diffusion transformer", "motion synthesis", "computer vision"],
    "isAccessibleForFree": true,
    "license": "https://creativecommons.org/licenses/by/4.0/"
  }
  </script>
</head>
<body>


  <!-- Scroll to Top Button -->
  <button class="scroll-to-top" onclick="scrollToTop()" title="Scroll to top" aria-label="Scroll to top">
    <i class="fas fa-chevron-up"></i>
  </button>

  <!-- More Works Dropdown -->

  <main id="main-content">
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">EchoMotion: Unified Human Video and Motion Generation via Dual-Modality Diffusion Transformer</h1>
            <!-- AUTHOR SECTION (Corrected HTML Structure) -->
            <div class="column has-text-centered">

            <!-- Author Names (Bolding Removed) -->
            <div class="is-size-5 publication-authors">
              <span class="author-block"><a href="https://yuxiaoyang23.github.io/" target="_blank">Yuxiao Yang</a><sup>1,2</sup>,</span>
              <span class="author-block"><a href="https://scholar.google.com/citations?user=73JaDUQAAAAJ&hl=zh-CN" target="_blank">Hualian Sheng</a><sup>2</sup>,</span>
              <span class="author-block"><a href="https://scholar.google.com/citations?user=LMVeRVAAAAAJ&hl=en" target="_blank">Sijia Cai</a><sup>2,*</sup>,</span>
              <span class="author-block"><a href="https://jinglin7.github.io/" target="_blank">Jing Lin</a><sup>3</sup>,</span>
              <br>
              <!-- Removed class="author-bold" from all links below -->
              <span class="author-block"><a href="https://scholar.google.com/citations?user=zQnTBEoAAAAJ&hl=en" target="_blank">Jiahao Wang</a><sup>4</sup>,</span>
              <span class="author-block"><a href="https://scholar.google.com/citations?user=VQp_ye4AAAAJ&hl=zh-CN" target="_blank">Bing Deng</a><sup>2</sup>,</span>
              <span class="author-block"><a href="https://scholar.google.com/citations?user=907PxdcAAAAJ&hl=en" target="_blank">Junzhe Lu</a><sup>1</sup>,</span>
              <span class="author-block"><a href="https://scholar.google.com/citations?hl=zh-CN&user=eldgnIYAAAAJ&view_op=list_works&sortby=pubdate" target="_blank">Haoqian Wang</a><sup>1,†</sup>,</span>
              <span class="author-block"><a href="https://scholar.google.com/citations?user=T9AzhwcAAAAJ&hl=en" target="_blank">Jieping Ye</a><sup>2,†</sup></span>
            </div>


            <!-- Affiliations and Symbols (Corrected) -->
            <div class="is-size-5 publication-authors" style="margin-top: 1rem;">
              <span class="author-block">
                  <sup>1</sup>Tsinghua University&nbsp;&nbsp;&nbsp;
                  <sup>2</sup>Alibaba Group&nbsp;&nbsp;&nbsp;
                  <br>
                  <sup>3</sup>Nanyang Technological University&nbsp;&nbsp;&nbsp;
                  <sup>4</sup>Xi'an Jiaotong University
              </span>
              <span class="is-size-5 eql-cntrb"><br> <!-- Correctly opened <small> tag -->
                  <sup>*</sup>Project Lead&nbsp;&nbsp;&nbsp;&nbsp;
                  <sup>†</sup>Corresponding Author
              </small></span> <!-- Correctly closed </small> tag -->
            </div>  

            <div class="column has-text-centered">
            
                <!-- arXiv Paper -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2512.18814" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>

                <!-- Code Button (Coming Soon) -->
                <span class="link-block">
                  <a class="external-link button is-normal is-rounded is-dark" disabled>
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code
                      <span style="font-size: 0.8em; opacity: 0.9; font-style: italic;">&nbsp;(Coming Soon)</span>
                    </span>
                  </a>
                </span>
            
                
                <!-- Dataset Button -->
                <span class="link-block">
                  <a class="external-link button is-normal is-rounded is-dark" disabled>
                    <span class="icon">
                      <i class="fas fa-database"></i> 
                    </span>
                    <span>Dataset
                      <span style="font-size: 0.8em; opacity: 0.9; font-style: italic;">&nbsp;(Coming Soon)</span>
                    </span>
                  </a>
                </span>

            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

  <!-- Overview / Motivation Section -->
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3 has-text-centered">Overview</h2>
          <div class="content has-text-justified">
            <p>
              <strong>Why do state-of-the-art video generation models often fail on complex human actions?</strong> While recent models excel at generating visually stunning scenes, they frequently struggle to synthesize plausible and coherent human movements. The reason lies in their training objective: by focusing solely on pixel-level fidelity, these models learn to mimic appearances but fail to grasp the underlying kinematic principles of human articulation. This leads to artifacts like floating feet and unnatural limb movements.
            </p>
            <p>
              To overcome this, we introduce <strong>EchoMotion</strong>, a new framework that fundamentally changes the learning paradigm. Instead of treating video as just a sequence of pixels, we propose to <strong>jointly model appearance and the explicit human motion that drives it</strong>. Our core idea is that by providing the model with a clear understanding of kinematic laws, we can significantly improve the coherence and realism of generated human-centric videos. EchoMotion is designed to learn the joint distribution of what we see (appearance) and how it moves (motion).
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>

    <!-- Method Section -->
    <section class="section">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column is-full">
            <h2 class="title is-3 has-text-centered">Method</h2>
            <div class="content has-text-justified">
              <p class="has-text-centered">
               At the heart of EchoMotion is a <strong>dual-branch Diffusion Transformer (DiT) architecture</strong> that processes visual and motion information in parallel. This design allows the model to learn rich cross-modal correlations.
              </p>
              <h4 class="title is-4" style="margin-top: 2rem;">Our key technical contributions include:</h4>
              <img src="static/images/pipeline.png" alt="EchoMotion Framework" style="width: 100%; max-width: 1200px; display: block; margin: 2rem auto; border-radius: 10px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);">
              <ul>
                <li>
                  <strong>Dual-Stream Architecture with Motion Latents:</strong> 
                  We first encode raw SMPL motion data into a compact latent representation. These motion tokens are then concatenated with visual tokens from the video frames. Our dual-stream DiT processes this combined sequence, enabling deep fusion of appearance and kinematic information within its attention layers.
                </li>
                <li style="margin-top: 1rem;">
                  <strong>MVS-RoPE (Motion-Video Synchronized Positional Encoding):</strong> 
                  How can the model understand that a specific motion corresponds to a specific video frame? We introduce MVS-RoPE, a novel unified 3D positional encoding scheme. It provides a shared coordinate system for both video and motion tokens, creating a powerful inductive bias that encourages temporal alignment between the two modalities. This ensures that the generated motion perfectly syncs with the visual output.
                </li>
              </ul>
              <img src="static/images/training.png" alt="MVS-RoPE" style="width: 100%; max-width: 1200px; display: block; margin: 2rem auto; border-radius: 10px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);">
              <ul>
                <li style="margin-top: 1rem;">
                  <strong>Motion-Video Two-Stage Training Strategy:</strong>
                  We design a flexible two-stage training strategy. In the first stage, the model learns to generate motion from text. In the second stage, it learns to generate video conditioned on both text and the pre-trained motion priors. This strategy not only enables joint generation but also unlocks versatile capabilities like motion-to-video and video-to-motion translation, all within a single unified model.
                </li>
              </ul>
            </div>
          </div>
        </div>
      </div>
    </section>


  <!-- HuMoVe Dataset Section -->
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column is-full">
          <h2 class="title is-3 has-text-centered">The HuMoVe Dataset</h2>
          <div class="content has-text-justified">
            <p>
              Training a model like EchoMotion requires a large-scale, high-quality dataset of paired video and motion data. To this end, we constructed <strong>HuMoVe</strong>, the first dataset of its kind, containing approximately <strong>80,000 video-motion pairs</strong>.
            </p>
            <img src="static/images/HuMoVe.png" alt="HuMoVe Dataset Samples" style="width: 100%; max-width: 1200px; display: block; margin: 2rem auto; border-radius: 10px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);">
            <h4 class="title is-4" style="margin-top: 2rem;">Highlights:</h4>
             <ul>
              <li><strong>Wide Category Coverage:</strong> HuMoVe spans a diverse range of human activities, from daily actions and sports to complex dance performances, ensuring our model learns a robust and generalizable representation of human motion.</li>
              <li><strong>High-Quality Annotations:</strong> Each video is accompanied by both a detailed textual description and a precise SMPL motion sequence, extracted using state-of-the-art motion capture techniques.</li>
              <li><strong>High-Fidelity Videos:</strong> We meticulously curated high-resolution, clean video clips, free from major occlusions or distracting backgrounds, providing an ideal training ground for generation models.</li>
            </ul>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- Results Section -->
  <section class="section">
    <div class="container is-max-desktop">
      <h3 class="title is-3 has-text-centered">Results</h3>

      <!-- Part 1: Text to Joint Video-and-Motion Generation -->
      <h4 class="title is-4" style="margin-top: 3rem; text-decoration: underline;">Text to Joint Video-and-Motion Generation</h4>
      <div class="content">
        <h4 class="title is-5 has-text-centered" style="margin-top: 2rem;">Quantitative Comparison</h4>
        <p>We first evaluate EchoMotion on generating a video and its corresponding motion sequence simultaneously from a text prompt. Compared to baseline models that only generate video, EchoMotion produces significantly more coherent and physically plausible human movements.</p>
        <img src="static/images/tablet2v.png" alt="Quantitative Comparison Table" style="width: 100%; max-width: 1200px; display: block; margin: 2rem auto;">
        <h4 class="title is-5 has-text-centered" style="margin-top: 2rem;">Visual Comparison</h4>


        <!-- Case 1 -->
        <div style="margin-top: 2rem;">
          <!-- Prompt 1 -->
          <p class="has-text-centered" style="font-style: italic;">
            "The video shows a skateboarder quickly sliding down a stair railing, maintaining balance throughout, with bold and smooth movements."
          </p>
          <!-- 视频对比 -->
          <div class="columns is-vcentered is-centered">
            <!-- 左侧视频 -->
            <div class="column">
              <video poster="static/images/baseline_poster_case1.jpg" autoplay controls muted loop preload="metadata" style="width: 100%;">
                <source src="static/videos/mwv_compare/case1_basemodel.mp4" type="video/mp4">
              </video>
            </div>
            <!-- 右侧视频 -->
            <div class="column">
              <video poster="static/images/echomotion_poster_case1.jpg" autoplay controls muted loop preload="metadata" style="width: 100%;">
                <source src="static/videos/mwv_compare/case1_ours.mp4" type="video/mp4">
              </video>
            </div>
          </div>

          <!-- Prompt 2 -->
          <p class="has-text-centered" style="font-style: italic;">
            "A gymnast is stretching on the mat before training, lifting her legs over her head and showing off her incredible flexibility."
          </p>
          <!-- 视频对比 -->
          <div class="columns is-vcentered is-centered">
            <!-- 左侧视频 -->
            <div class="column">
              <video poster="" autoplay controls muted loop preload="metadata" style="width: 100%;">
                <source src="static/videos/mwv_compare/case2_basemodel.mp4" type="video/mp4">
              </video>
              <p class="has-text-centered has-text-weight-bold" style="margin-top: 0.5rem;">Wan5B</p>
            </div>
            <!-- 右侧视频 -->
            <div class="column">
              <video poster="" autoplay controls muted loop preload="metadata" style="width: 100%;">
                <source src="static/videos/mwv_compare/case2_ours.mp4" type="video/mp4">
              </video>
              <p class="has-text-centered has-text-weight-bold" style="margin-top: 0.5rem;">EchoMotion(Wan5B)</p>
            </div>
          </div>
        </div>



      <!-- Part 2: Motion-to-Video Generation -->
      <h3 class="title is-4" style="margin-top: 3rem; text-decoration: underline;">Motion-to-Video Generation</h3>
      <div class="content">
        <p>Thanks to our versatile framework, EchoMotion can also generate a video conditioned on a given motion sequence (e.g., from a motion capture file). This allows for precise control over the generated character's actions.</p>
        
        <div id="m2v-carousel" class="carousel results-carousel" style="margin-top: 1rem;">
          
          <div class="item">
            <!-- 使用 columns 容器包裹两个视频 -->
            <div class="columns is-vcentered is-centered">
              <!-- 第一个视频列 -->
              <div class="column">
                <video poster="" autoplay controls muted loop preload="metadata"><source src="static/videos/mt2v/case1_motion.mp4" type="video/mp4"></video>
              </div>
              <!-- 新增：用于放置箭头的中间列 -->
              <div class="column is-narrow" style="display: flex; align-items: center; justify-content: center;">
                <span class="is-size-1 has-text-grey-light">→</span>
              </div>
              <!-- 第二个视频列 -->
              <div class="column">
                <video poster="" autoplay controls muted loop preload="metadata"><source src="static/videos/mt2v/case1_video.mp4" type="video/mp4"></video>
              </div>
            </div>
            <!-- 标题放在 columns 容器下方 -->
            <h2 class="subtitle has-text-centered" style="font-style: italic;">“An Asian young man is standing in the center of a dimly lit gym, wearing a dark gray hoodie and black tight training pants, engaged in strength training. The
              background features neatly arranged dark metal racks, which hold dumbbells and colored solid balls—orange, deep blue, and dark green spheres are interspersed, with a
              clearly visible brick wall texture in the back....”</h2>
          </div>
          
          <div class="item">
            <!-- 同样使用 columns 容器包裹 -->
            <div class="columns is-vcentered is-centered">
              <!-- 第一个视频列 -->
              <div class="column">
                <video poster="" autoplay controls muted loop preload="metadata"><source src="static/videos/mt2v/case2_motion.mp4" type="video/mp4"></video>
              </div>
                            <!-- 新增：用于放置箭头的中间列 -->
                            <div class="column is-narrow" style="display: flex; align-items: center; justify-content: center;">
                              <span class="is-size-1 has-text-grey-light">→</span>
                            </div>
              <!-- 第二个视频列 -->
              <div class="column">
                <video poster="" autoplay controls muted loop preload="metadata"><source src="static/videos/mt2v/case2_video.mp4" type="video/mp4"></video>
              </div>
            </div>
            <!-- 标题放在 columns 容器下方 -->
            <h2 class="subtitle has-text-centered" style="font-style: italic;">“A young South Asian male is doing physical training on a rural path at dusk. He has a lean and muscular build, short hair, and a light beard. He is wearing a dark gray
              sleeveless sports vest and navy blue shorts, decorated with light gray stripes at the hem. He is barefoot on the dirt path. The background features an open, fallow
              farmland, with dried straw scattered on the ground....”</h2>
          </div>

          <div class="item">
            <!-- 同样使用 columns 容器包裹 -->
            <div class="columns is-vcentered is-centered">
              <!-- 第一个视频列 -->
              <div class="column">
                <video poster="" autoplay controls muted loop preload="metadata"><source src="static/videos/mt2v/case3_motion.mp4" type="video/mp4"></video>
              </div>
                            <!-- 新增：用于放置箭头的中间列 -->
                            <div class="column is-narrow" style="display: flex; align-items: center; justify-content: center;">
                              <span class="is-size-1 has-text-grey-light">→</span>
                            </div>
              <!-- 第二个视频列 -->
              <div class="column">
                <video poster="" autoplay controls muted loop preload="metadata"><source src="static/videos/mt2v/case3_video.mp4" type="video/mp4"></video>
              </div>
            </div>
            <!-- 标题放在 columns 容器下方 -->
            <h2 class="subtitle has-text-centered" style="font-style: italic;">“A young Asian woman is dressed in a dark turtleneck knit sweater and loose-fitting trousers, set in a minimalist industrial-style indoor space. The background features a
              textured concrete wall and a large floor-to-ceiling window. The shot is a medium shot, stable and fixed, with black-and-white tones maintaining the original high-contrast
              style, and the light and shadow structures remain unchanged, creating a calm yet tense overall atmosphere....”</h2>
          </div>

          <div class="item">
            <!-- 同样使用 columns 容器包裹 -->
            <div class="columns is-vcentered is-centered">
              <!-- 第一个视频列 -->
              <div class="column">
                <video poster="" autoplay controls muted loop preload="metadata"><source src="static/videos/mt2v/case4_motion.mp4" type="video/mp4"></video>
              </div>
                            <!-- 新增：用于放置箭头的中间列 -->
                            <div class="column is-narrow" style="display: flex; align-items: center; justify-content: center;">
                              <span class="is-size-1 has-text-grey-light">→</span>
                            </div>
              <!-- 第二个视频列 -->
              <div class="column">
                <video poster="" autoplay controls muted loop preload="metadata"><source src="static/videos/mt2v/case4_video.mp4" type="video/mp4"></video>
              </div>
            </div>
            <!-- 标题放在 columns 容器下方 -->
            <h2 class="subtitle has-text-centered" style="font-style: italic;">“A deep-skinned, athletic African female athlete is in the final phase of her run-up for long jump at an outdoor track and field venue. She is on a deep red rubber track,
              with light yellow marker blocks visible at the edge of the track. The background features slightly blurred palm trees and shrubs, interspersed with scattered orange
              tropical flowers, and in the distance, there is the silhouette of low gray stands......”</h2>
          </div>

          <div class="item">
            <!-- 同样使用 columns 容器包裹 -->
            <div class="columns is-vcentered is-centered">
              <!-- 第一个视频列 -->
              <div class="column">
                <video poster="" autoplay controls muted loop preload="metadata"><source src="static/videos/mt2v/case5_motion.mp4" type="video/mp4"></video>
              </div>
                            <!-- 新增：用于放置箭头的中间列 -->
                            <div class="column is-narrow" style="display: flex; align-items: center; justify-content: center;">
                              <span class="is-size-1 has-text-grey-light">→</span>
                            </div>
              <!-- 第二个视频列 -->
              <div class="column">
                <video poster="" autoplay controls muted loop preload="metadata"><source src="static/videos/mt2v/case5_video.mp4" type="video/mp4"></video>
              </div>
            </div>
            <!-- 标题放在 columns 容器下方 -->
            <h2 class="subtitle has-text-centered" style="font-style: italic;">“A young woman is bouncing on an indoor trampoline, with a purple wall in the background, which features two tall arched windows symmetrically placed …”</h2>
          </div>



          <!-- 第二个滑块内容 -->
          <div class="item">
            <!-- 新增：创建一个居中的列容器来包裹所有内容 -->
            <div class="columns is-centered">
              <!-- 新增：这个单列将控制其内部所有内容的宽度 -->
              <!-- 关键改动：使用 is-half (50%宽度) 来让视频变窄 -->
              <div class="column is-half">

                <!-- 您的原始内容，顺序和样式保持不变 -->
                <video poster="" autoplay controls muted loop preload="metadata"><source src="static/videos/mt2v/case6_motion.mp4" type="video/mp4"></video>
                <h2 class="subtitle has-text-centered" style="font-style: italic;">“A young Asian woman is jogging by the lakeside in the morning. Her long dark brown hair is tied
                  in a high ponytail, and she is wearing black wireless sports headphones with a smartwatch on her
                  wrist. She is dressed in a dark gray athletic outfit, including a short-sleeved fitted top and highwaisted
                  quick-dry leggings, highlighting her sleek body lines. Her expression is relaxed, with a
                  gentle smile, eyes focused ahead. The background features a serene lake in the soft morning light,
                  with ripples of pale blue and silvery gray on the water. In the distance, the horizon is faintly visible,
                  and the sky shows a soft pinkish-purple morning glow.”</h2>
                <div class="has-text-centered" style="margin: 0.5rem 0;">
                  <span class="is-size-1 has-text-grey-light">↓</span>
                </div>
                <video poster="" autoplay controls muted loop preload="metadata" style="margin-top: 1rem;"><source src="static/videos/mt2v/case6_video.mp4" type="video/mp4"></video>
                
              </div>
            </div>
          </div>


          <!-- 第二个滑块内容 -->
          <div class="item">
            <!-- 新增：创建一个居中的列容器来包裹所有内容 -->
            <div class="columns is-centered">
              <!-- 新增：这个单列将控制其内部所有内容的宽度 -->
              <!-- 关键改动：使用 is-half (50%宽度) 来让视频变窄 -->
              <div class="column is-half">

                <!-- 您的原始内容，顺序和样式保持不变 -->
                <video poster="" autoplay controls muted loop preload="metadata"><source src="static/videos/mt2v/case7_motion.mp4" type="video/mp4"></video>
                <h2 class="subtitle has-text-centered" style="font-style: italic;">“An East Asian boy of about ten to twelve years old is in a karate dojo, his expression focused and c
                  omposed. He has neatly trimmed black hair and a determined gaze directed forward, showing a calm
                  ness and perseverance beyond his age. He is wearing a clean white karate uniform with a red belt ar
                  ound his waist, indicating an advanced level. The uniform’s fabric is stiff. The scene is set in a well-li
                  t indoor dojo, with light oak flooring reflecting soft light. The background is slightly blurred, reveali
                  ng purple and gray padded protective gear along the walls and a few black balance balls resting nea
                  rby. On the right side, a wide glass window lets in morning natural light.”</h2>
                <div class="has-text-centered" style="margin: 0.5rem 0;">
                  <span class="is-size-1 has-text-grey-light">↓</span>
                </div>
                <video poster="" autoplay controls muted loop preload="metadata" style="margin-top: 1rem;"><source src="static/videos/mt2v/case7_video.mp4" type="video/mp4"></video>
                
              </div>
            </div>
          </div>



          </div>
        </div>
      </div>


      <!-- Part 3: Video-to-Motion Prediction -->
      <h3 class="title is-4" style="margin-top: 3rem; text-decoration: underline;">Video-to-Motion Prediction</h3>
      <p>As a reverse task, EchoMotion can predict the underlying 3D motion sequence from an input video. This showcases the model's deep understanding of the relationship between appearance and kinematics.</p>
        <div id="m2v-carousel" class="carousel results-carousel" style="margin-top: 1rem;">
          
          <div class="item">
            <div class="columns is-vcentered is-centered">
              <!-- 第一个视频列 -->
              <div class="column">
                <video poster="" autoplay controls muted loop preload="metadata"><source src="static/videos/v2m/case1_video.mp4" type="video/mp4"></video>
              </div>
                            <!-- 新增：用于放置箭头的中间列 -->
                            <div class="column is-narrow" style="display: flex; align-items: center; justify-content: center;">
                              <span class="is-size-1 has-text-grey-light">→</span>
                            </div>
              <!-- 第二个视频列 -->
              <div class="column">
                <video poster="" autoplay controls muted loop preload="metadata"><source src="static/videos/v2m/case1_motion.mp4" type="video/mp4"></video>
              </div>
            </div>
          </div>
          
          <!-- <div class="item">
            <div class="columns is-vcentered is-centered">

              <div class="column">
                <video poster="" autoplay controls muted loop preload="metadata"><source src="static/videos/v2m/case2_video.mp4" type="video/mp4"></video>
              </div>

              <div class="column">
                <video poster="" autoplay controls muted loop preload="metadata"><source src="static/videos/v2m/case2_motion.mp4" type="video/mp4"></video>
              </div>
            </div>
          </div> -->

          <div class="item">
            <div class="columns is-vcentered is-centered">
              <!-- 第一个视频列 -->
              <div class="column">
                <video poster="" autoplay controls muted loop preload="metadata"><source src="static/videos/v2m/case3_video.mp4" type="video/mp4"></video>
              </div>
                            <!-- 新增：用于放置箭头的中间列 -->
                            <div class="column is-narrow" style="display: flex; align-items: center; justify-content: center;">
                              <span class="is-size-1 has-text-grey-light">→</span>
                            </div>
              <!-- 第二个视频列 -->
              <div class="column">
                <video poster="" autoplay controls muted loop preload="metadata"><source src="static/videos/v2m/case3_motion.mp4" type="video/mp4"></video>
              </div>
            </div>
          </div>

          <div class="item">
            <div class="columns is-vcentered is-centered">
              <!-- 第一个视频列 -->
              <div class="column">
                <video poster="" autoplay controls muted loop preload="metadata"><source src="static/videos/v2m/case4_video.mp4" type="video/mp4"></video>
              </div>
                            <!-- 新增：用于放置箭头的中间列 -->
                            <div class="column is-narrow" style="display: flex; align-items: center; justify-content: center;">
                              <span class="is-size-1 has-text-grey-light">→</span>
                            </div>
              <!-- 第二个视频列 -->
              <div class="column">
                <video poster="" autoplay controls muted loop preload="metadata"><source src="static/videos/v2m/case4_motion.mp4" type="video/mp4"></video>
              </div>
            </div>
          </div>

        </div>
            
    </div>
  </section>


    <!-- BibTeX Section -->
    <section class="section" id="BibTeX">
      <div class="container is-max-desktop content">
        <div class="bibtex-header">
          <h2 class="title">BibTeX</h2>
          <button class="copy-bibtex-btn" onclick="copyBibTeX()" title="Copy BibTeX to clipboard">
            <i class="fas fa-copy"></i>
            <span class="copy-text">Copy</span>
          </button>
        </div>
        <pre id="bibtex-code"><code>@article{yang2024echomotion,
    title={EchoMotion: Unified Human Video and Motion Generation via Dual-Modality Diffusion Transformer},
    author={Yang, Yuxiao and Sheng, Hualian and Cai, Sijia and Lin, Jing and Wang, Jiahao and Deng, Bing and Lu, Junzhe and Wang, Haoqian and Ye, Jieping},
    journal={arXiv preprint arXiv:2512.18814},
    year={2024}
  }</code></pre>
      </div>
    </section>


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
